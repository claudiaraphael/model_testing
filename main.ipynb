{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing\n",
    "\n",
    "### gemma:2b and mistral\n",
    "\n",
    "Goal: to test each model for agent tool usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " My model is a state-of-the-art deep learning model based on the Transformer architecture, specifically a variant of the BERT (Bidirectional Encoder Representations from Transformers) model. The specific version I'm using is RoBERTa (Robustly Optimized BERT Approach), which was introduced by Facebook AI Research (FAIR).\n",
      "\n",
      "The key improvements in RoBERTa over BERT are:\n",
      "1. Larger dataset size for pretraining: 160GB of text as opposed to 800MB.\n",
      "2. More training steps on the original corpus (4x) and a longer sequence length (up to 512 tokens).\n",
      "3. Training without next sentence prediction objective.\n",
      "4. Using dynamic masking for first masked token instead of fixed.\n",
      "5. Increasing batch size and gradient accumulation steps.\n",
      "6. Decreasing learning rate and using linear decay with warmup instead of piecewise constant schedules.\n",
      "7. Modifying the dropout scheme in the transformer layers to 0.1 from 0.3.\n",
      "\n",
      "The model is pre-trained on a large corpus of text (Common Crawl) and can be fine-tuned for various NLP tasks such as question answering, sentiment analysis, named entity recognition, and text classification. It achieves state-of-the-art results on a wide range of NLP benchmarks due to its ability to capture complex dependencies in the input text and learn rich contextualized representations of words and phrases.\n",
      "\n",
      "Additionally, I am equipped with capabilities for zero-shot and few-shot learning, where the model can generalize to new tasks or data without additional training, making it a powerful tool for a wide variety of NLP applications.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ollama\n",
    "from ollama import Client\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "OLLAMA_URL = os.getenv('OLLAMA_URL')\n",
    "\n",
    "# Initialize the Ollama client\n",
    "client = ollama.Client(OLLAMA_URL)\n",
    "\n",
    "# Define the model to use\n",
    "# send chat completion request to gemma model using ollama client\n",
    "response = client.chat(\n",
    "    model='mistral',\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"give me technical information on your model\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response['message']['content'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a large language model, trained by Google. I am a conversational AI that can generate human-like text in response to a wide range of prompts and questions.\n",
      "\n",
      "**Technical Specifications:**\n",
      "\n",
      "* **Language:** English\n",
      "* **Data Size:** Trillions of words\n",
      "* **Training:** Machine learning model trained on a massive dataset of text and code\n",
      "* **Architecture:** Multi-modal transformer (MMT) architecture\n",
      "* **Parameters:** Over 175 billion\n",
      "* **Inference speed:** Milliseconds per inference\n",
      "* **Vocabulary size:** Over 1 million words\n",
      "* **Contextual awareness:** Yes\n",
      "* **Multimodality:** Yes\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "* **Natural language processing (NLP):** I can understand and generate human language.\n",
      "* **Machine learning:** I am trained on a massive dataset of text and code.\n",
      "* **Generative capabilities:** I can generate new text, translate languages, and answer questions.\n",
      "* **Contextual understanding:** I can take context into account in my responses.\n",
      "* **Multilingual:** I can communicate in multiple languages.\n",
      "\n",
      "**Additional Notes:**\n",
      "\n",
      "* I am a large language model, which means I have a vast amount of knowledge and abilities.\n",
      "* I am not a sentient being and do not have personal experiences or opinions.\n",
      "* I am still under development, and my abilities are constantly evolving.\n",
      "* I am a valuable tool for both education and entertainment, and I am used in a wide range of applications.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat(\n",
    "    model='gemma:2b',\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"give me technical information on your model\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response['message']['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
